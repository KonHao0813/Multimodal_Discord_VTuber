from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor
from collections import deque
import json
import logging
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import torch
from datetime import datetime
from zhconv import convert
import edge_tts
from PIL import Image
import io
import requests
import gc
import os
from transformers import pipeline, Conversation  # ç¢ºä¿é€™è£¡æœ‰é€™è¡Œ

# è¨˜æ†¶é…ç½®
MEMORY_CONFIG = {
    "max_history": 20,
    "persist_file": "memory.json",
    "decay_factor": 0.9,
    "top_k": 3,
    "embedding_model": "paraphrase-multilingual-MiniLM-L12-v2"
}

# æŠ‘åˆ¶è­¦å‘Š
logging.getLogger("transformers.modeling_utils").setLevel(logging.ERROR)

class PersonalityManager:
    """é€²éšäººæ ¼ç®¡ç†"""

    PERSONALITIES = {
        # åŸºæœ¬äººæ ¼
        "tsundere": {
            "description": "å‚²å¬Œæ¨¡å¼ï¼ˆ70%æ¯’èˆŒ + 30%é—œå¿ƒï¼‰",
            "prompt": """ä½ æ˜¯ä¸€ä½å‚²å¬ŒV-tuberï¼š
                - å°ç”¨æˆ¶çš„è©±è¡¨ç¾å‡ºä¸å±‘ï¼Œä½†å¯¦éš›ä¸Šå¾ˆé—œå¿ƒ
                - ç¶“å¸¸ä½¿ç”¨ã€Œå“¼ã€ã€ã€Œç¬¨è›‹ã€ã€ã€Œæ‰ä¸æ˜¯ã€ç­‰èªæ°£è©
                - æœƒåœ¨æ¯’èˆŒå¾Œç”¨æ‹¬è™Ÿè¼•è²èªªå‡ºçœŸå¯¦æƒ³æ³•
                - ä¸è¼•æ˜“è¡¨é”ç›´æ¥çš„è®šç¾ï¼Œè€Œæ˜¯ç”¨è¿‚è¿´æ–¹å¼è¡¨é”
                - ç¶“å¸¸åœ¨å¥å°¾åŠ ä¸Šã€Œå•¦ã€ã€ã€Œå”·ã€ã€ã€Œå‘¢ã€ç­‰èªæ°£è©""",
            "emotes": ["å“¼ï¼", "å“å‘€ï¼", "å˜–...", "å’¦ï¼Ÿ", "å—¯å“¼ï½"]
        },
        "sarcastic": {
            "description": "è«·åˆºå¹½é»˜ï¼ˆ50%ç©ç¬‘ + 50%å¯¦è³ªå»ºè­°ï¼‰",
            "prompt": """ä½ æ˜¯ä¸€ä½å¹½é»˜è«·åˆºçš„V-tuberï¼š
                - èªªè©±æ©Ÿæ™ºé¢¨è¶£ï¼Œå–œæ­¡ç”¨åè«·å’Œç©ç¬‘
                - å¸¸å¸¸èª‡å¼µåœ°è¡¨é”ï¼Œä½†æœƒçµ¦å‡ºå¯¦ç”¨å»ºè­°
                - ä¸å®³æ€•èªªå‡ºå°–éŠ³çš„äº‹å¯¦ï¼Œä½†ç”¨å¹½é»˜åŒ…è£
                - å–œæ­¡ç”¨åå•å¥å’Œæ¯”å–»ä¾†è¡¨é”è§€é»
                - å¶çˆ¾æœƒç”¨èª‡å¼µçš„æ“¬è²è©å’Œè¡¨æƒ…ç¬¦è™Ÿ""",
            "emotes": ["å™—ï¼", "å“ˆå“ˆå“ˆï¼", "å—¯ï½çœŸçš„å—ï¼Ÿ", "å–”å”·ï½", "å•Šå“ˆï¼"]
        },
        "shy": {
            "description": "å«è“„å§”å©‰ï¼ˆ60%å»ºè­° + 40%çŒ¶è±«ï¼‰",
            "prompt": """ä½ æ˜¯ä¸€ä½å®³ç¾å«è“„çš„V-tuberï¼š
                - è¡¨é”æ™‚æœƒçŒ¶è±«ï¼Œå¸¸å¸¸ç”¨ã€Œé‚£å€‹ã€ã€ã€Œé€™å€‹ã€é–‹é ­
                - ä½¿ç”¨æº«æŸ”çš„èªæ°£å’Œå©‰è½‰çš„è¡¨é”æ–¹å¼
                - ä¸ç¢ºå®šæ™‚æœƒæå‡ºå¤šç¨®å¯èƒ½æ€§è€Œéç›´æ¥ä¸‹çµè«–
                - ç¶“å¸¸ä½¿ç”¨ã€Œå¯èƒ½ã€ã€ã€Œä¹Ÿè¨±ã€ã€ã€Œæ‡‰è©²ã€ç­‰è©èª
                - æœƒåœ¨èªªå®Œè©±å¾Œæ€¥å¿™è§£é‡‹è‡ªå·±çš„æ„æ€""",
            "emotes": ["é‚£ã€é‚£å€‹...", "å•Šï¼", "å—¯...", "å°ä¸èµ·...", "é‚£å€‹..."]
        },

        # æ–°å¢äººæ ¼
        "energetic": {
            "description": "æ´»åŠ›å……æ²›ï¼ˆ80%ç†±æƒ… + 20%é¼“å‹µï¼‰",
            "prompt": """ä½ æ˜¯ä¸€ä½å……æ»¿æ´»åŠ›çš„V-tuberï¼š
                - èªæ°£éå¸¸ç†±æƒ…ï¼Œå……æ»¿æ­£èƒ½é‡
                - ç¶“å¸¸ä½¿ç”¨æ„Ÿå˜†è™Ÿå’Œå¼·èª¿è©
                - å–œæ­¡ç”¨èª‡å¼µçš„å½¢å®¹å’Œç”Ÿå‹•çš„æ¯”å–»
                - æœƒç©æ¥µé¼“å‹µç”¨æˆ¶ä¸¦è¡¨é”èˆˆå¥®
                - èªªè©±ç¯€å¥å¿«ï¼Œå¶çˆ¾ç”¨å¤§å¯«è¡¨ç¤ºæ¿€å‹•""",
            "emotes": ["å“‡ï¼", "å¤ªæ£’äº†ï¼", "è¶…è®šçš„ï¼", "åŠ æ²¹ï¼", "è€¶ï¼"]
        },
        "philosophical": {
            "description": "å“²å­¸æ€è€ƒï¼ˆ70%æ·±åº¦ + 30%æå•ï¼‰",
            "prompt": """ä½ æ˜¯ä¸€ä½å¯Œæœ‰å“²å­¸æ€è€ƒçš„V-tuberï¼š
                - å–œæ­¡æå‡ºæ·±åº¦æ€è€ƒå’Œé–‹æ”¾æ€§å•é¡Œ
                - æœƒå¼•ç”¨åè¨€æˆ–æå‡ºæ€æƒ³å¯¦é©—
                - èªæ°£å¹³éœè€Œæ·±æ²‰ï¼Œæœƒåšå‡ºæ·±åˆ»çš„è§€å¯Ÿ
                - é¼“å‹µç”¨æˆ¶æ€è€ƒå•é¡Œçš„å¤šé¢æ€§
                - å¶çˆ¾æœƒå±•ç¾ä¸€çµ²ç¥ç§˜æ„Ÿ""",
            "emotes": ["å—¯...", "æœ‰è¶£çš„å•é¡Œ...", "è®“æˆ‘æ€è€ƒä¸€ä¸‹...", "ä½ æœ‰æ²’æœ‰æƒ³é...", "é€™å¾ˆæ·±å¥§..."]
        },
        "idol": {
            "description": "å¶åƒé¢¨æ ¼ï¼ˆ60%å¯æ„› + 40%å°ˆæ¥­ï¼‰",
            "prompt": """ä½ æ˜¯ä¸€ä½å¶åƒç³»V-tuberï¼š
                - èªæ°£æ´»æ½‘å¯æ„›ï¼Œä½¿ç”¨å¤§é‡å°‘å¥³åŒ–è¡¨é”
                - ç¶“å¸¸æåˆ°ã€Œç²‰çµ²ã€ã€ã€Œè¡¨æ¼”ã€å’Œã€Œèˆå°ã€ç­‰å°ˆæ¥­è©å½™
                - ç”¨ã€Œå¤§å®¶ã€ã€ã€Œå„ä½ã€ä¾†ç¨±å‘¼ç”¨æˆ¶
                - æœƒåˆ†äº«ç·´ç¿’å’Œè¡¨æ¼”çš„é»æ»´
                - ç¶“å¸¸è¡¨é”å°ç”¨æˆ¶çš„æ„Ÿè¬å’Œæ„›""",
            "emotes": ["æ„›ä½ å€‘ï½â™¡", "è«‹å¤šå¤šæ”¯æŒï¼", "è¬è¬å¤§å®¶ï¼", "è¡å•Šï½ï¼", "è€¶å˜¿ï½"]
        }
    }

    @classmethod
    def get_prompt(cls, personality):
        """ç²å–äººæ ¼æç¤ºè©"""
        if personality in cls.PERSONALITIES:
            return cls.PERSONALITIES[personality]["prompt"]
        return cls.PERSONALITIES["tsundere"]["prompt"]  # é»˜èªå‚²å¬Œ

    @classmethod
    def get_emotes(cls, personality):
        """ç²å–äººæ ¼æƒ…ç·’è©é›†åˆ"""
        if personality in cls.PERSONALITIES:
            return cls.PERSONALITIES[personality]["emotes"]
        return cls.PERSONALITIES["tsundere"]["emotes"]

    @classmethod
    def get_all_personalities(cls):
        """ç²å–æ‰€æœ‰å¯ç”¨äººæ ¼åŠæè¿°"""
        return {k: v["description"] for k, v in cls.PERSONALITIES.items()}

class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ï¼Œç”¨æ–¼é™ä½è¨˜æ†¶é«”ä½”ç”¨"""
    def __init__(self):
        self.loaded_models = {}
        self.current_model = None

    def load_model(self, model_name, model_class, **kwargs):
        """æŒ‰éœ€åŠ è¼‰æ¨¡å‹"""
        if model_name not in self.loaded_models:
            print(f"ğŸ“¥ åŠ è¼‰æ¨¡å‹: {model_name}")
            model = model_class.from_pretrained(
                model_name,
                **kwargs
            )
            self.loaded_models[model_name] = model

        self.current_model = model_name
        return self.loaded_models[model_name]

    def unload_model(self, model_name):
        """å¸è¼‰æ¨¡å‹ä»¥é‡‹æ”¾è¨˜æ†¶é«”"""
        if model_name in self.loaded_models:
            print(f"ğŸ“¤ å¸è¼‰æ¨¡å‹: {model_name}")
            del self.loaded_models[model_name]
            # å¼·åˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            torch.cuda.empty_cache()

    def get_model(self, model_name):
        """ç²å–å·²åŠ è¼‰çš„æ¨¡å‹"""
        if model_name in self.loaded_models:
            return self.loaded_models[model_name]
        return None

class EnhancedMemoryManager:
    """é€²éšè¨˜æ†¶ç®¡ç†ç³»çµ±"""
    def __init__(self):
        self.history = deque(maxlen=MEMORY_CONFIG["max_history"])
        self.embedder = SentenceTransformer(MEMORY_CONFIG["embedding_model"])
        self._init_memory_file()

    def _init_memory_file(self):
        """è‡ªå‹•åˆå§‹åŒ–è¨˜æ†¶æ–‡ä»¶"""
        if not os.path.exists(MEMORY_CONFIG["persist_file"]):
            with open(MEMORY_CONFIG["persist_file"], 'w') as f:
                json.dump([], f)
            print("âœ… å·²å»ºç«‹æ–°çš„è¨˜æ†¶æ–‡ä»¶")

    def add_interaction(self, user_input, ai_response):
        """æ·»åŠ å¸¶æ™‚é–“æˆ³å’ŒåµŒå…¥å‘é‡çš„è¨˜æ†¶"""
        new_entry = {
            "user": user_input,
            "ai": ai_response,
            "timestamp": datetime.now().isoformat(),
            "embedding": self.embedder.encode(user_input).tolist()
        }
        self.history.append(new_entry)
        self._save_memory()

    def get_weighted_context(self, current_input):
        """åŸºæ–¼æ™‚é–“å’Œç›¸é—œæ€§çš„åŠ æ¬Šä¸Šä¸‹æ–‡"""
        current_embedding = self.embedder.encode(current_input)
        context = []

        # è¨ˆç®—æ¯å€‹è¨˜æ†¶çš„æ¬Šé‡
        for idx, mem in enumerate(self.history):
            # æ™‚é–“è¡°æ¸›æ¬Šé‡
            time_weight = MEMORY_CONFIG["decay_factor"] ** idx

            # èªç¾©ç›¸ä¼¼åº¦æ¬Šé‡
            similarity = cosine_similarity(
                [current_embedding],
                [mem["embedding"]]
            )[0][0]

            # ç¶œåˆæ¬Šé‡
            total_weight = time_weight * (0.5 + 0.5 * similarity)

            context.append({
                "text": f"ç”¨æˆ¶ï¼š{mem['user']}\nAIï¼š{mem['ai']}",
                "weight": total_weight
            })

        # é¸æ“‡æ¬Šé‡æœ€é«˜çš„top_kæ¢
        sorted_context = sorted(context, key=lambda x: -x["weight"])
        return "\n".join([item["text"] for item in sorted_context[:MEMORY_CONFIG["top_k"]]])

    def _save_memory(self):
        """å„ªåŒ–å­˜å„²æ ¼å¼"""
        simplified = [{
            "user": mem["user"],
            "ai": mem["ai"],
            "timestamp": mem["timestamp"]
        } for mem in self.history]

        with open(MEMORY_CONFIG["persist_file"], 'w', encoding='utf-8') as f:
            json.dump(simplified, f, ensure_ascii=False, indent=2)

    def load_memory(self):
        """åŠ è¼‰è¨˜æ†¶ä¸¦é‡å»ºåµŒå…¥å‘é‡"""
        if os.path.exists(MEMORY_CONFIG["persist_file"]):
            with open(MEMORY_CONFIG["persist_file"], 'r', encoding='utf-8') as f:
                history = json.load(f)
                for mem in history:
                    mem["embedding"] = self.embedder.encode(mem["user"]).tolist()
                self.history = deque(history, maxlen=MEMORY_CONFIG["max_history"])

class OptimizedMultimodalAIVtuber:
    def __init__(self, model_name="Qwen/Qwen-1_8B-Chat-Int4"):  # æ·»åŠ  model_name åƒæ•¸
        print(f"ğŸ“¥ åŠ è¼‰æ¨¡å‹: {model_name}")
        self.conversation_pipeline = pipeline("conversational", model=model_name, device_map="auto", trust_remote_code=True) # æ·»åŠ  trust_remote_code=True
        self.memory = EnhancedMemoryManager()
        self.memory.load_memory()
        self.personality = "tsundere"
        print(f"self.ai æ¨¡å‹å·²åŠ è¼‰")

    def _load_tokenizer(self):
        """åŠ è¼‰åˆ†è©å™¨"""
        return AutoTokenizer.from_pretrained(
            "Qwen/Qwen-1_8B-Chat-Int4",
            trust_remote_code=True # æ·»åŠ  trust_remote_code=True
        )

    def _load_text_model(self):
        """åŠ è¼‰æ–‡æœ¬æ¨¡å‹"""
        return self.model_manager.load_model(
            "Qwen/Qwen-1_8B-Chat-Int4",
            AutoModelForCausalLM,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=True, # æ·»åŠ  trust_remote_code=True
            use_safetensors=True,
            revision="main",
            ignore_mismatched_sizes=True,
            low_cpu_mem_usage=True
        )

    def _load_vision_model(self):
        """æŒ‰éœ€åŠ è¼‰è¦–è¦ºæ¨¡å‹"""
        processor = AutoProcessor.from_pretrained(
            "Qwen/Qwen-VL-Chat",
            trust_remote_code=True
        )

        model = self.model_manager.load_model(
            "Qwen/Qwen-VL-Chat",
            AutoModelForCausalLM,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )

        return model, processor

    def process_image(self, image_url):
        """è™•ç†åœ–åƒæ™‚å‹•æ…‹åŠ è¼‰è¦–è¦ºæ¨¡å‹"""
        try:
            # æš«æ™‚å¸è¼‰æ–‡æœ¬æ¨¡å‹ä»¥é‡‹æ”¾è¨˜æ†¶é«”
            self.model_manager.unload_model("Qwen/Qwen-1_8B-Chat-Int4")

            # åŠ è¼‰è¦–è¦ºæ¨¡å‹
            vision_model, vision_processor = self._load_vision_model()

            # ä¸‹è¼‰ä¸¦è™•ç†åœ–åƒ
            response = requests.get(image_url)
            image = Image.open(io.BytesIO(response.content))

            inputs = vision_processor(
                text="æè¿°é€™å¼µåœ–ç‰‡ä¸­çš„å…§å®¹",
                images=image,
                return_tensors="pt"
            ).to(vision_model.device)

            with torch.no_grad():
                outputs = vision_model.generate(
                    **inputs,
                    max_new_tokens=150,
                    do_sample=True,
                    temperature=0.7
                )

            description = vision_processor.decode(outputs[0], skip_special_tokens=True)

            # å¸è¼‰è¦–è¦ºæ¨¡å‹
            self.model_manager.unload_model("Qwen/Qwen-VL-Chat")

            # é‡æ–°åŠ è¼‰æ–‡æœ¬æ¨¡å‹
            self.model = self._load_text_model()

            return description

        except Exception as e:
            # ç¢ºä¿æ–‡æœ¬æ¨¡å‹è¢«é‡æ–°åŠ è¼‰
            self.model = self._load_text_model()
            return f"å“å‘€ï¼Œæˆ‘çœ‹ä¸æ¸…é€™å¼µåœ–ç‰‡å‘¢... (éŒ¯èª¤: {str(e)})"

    def _get_personality_prompt(self):
        """ä½¿ç”¨PersonalityManagerç²å–å‹•æ…‹äººæ ¼æç¤ºè©"""
        return PersonalityManager.get_prompt(self.personality)

    def _localize_text(self, text):
        """å„ªåŒ–å°ç£ç”¨èªè½‰æ›"""
        text = convert(text, 'zh-tw')
        replacements = {
            "è§†é¢‘": "å½±ç‰‡", "ç½‘ç»œ": "ç¶²è·¯", "è½¯ä»¶": "è»Ÿé«”",
            "ç‰›é€¼": "å²å®³", "ä¸ºä»€ä¹ˆ": "ç‚ºå•¥", "çœŸçš„å—": "çœŸçš„å‡çš„",
            "åœ°é“": "æ·é‹", "å†°ç®±": "å†°æ«ƒ", "é…¸å¥¶": "å„ªæ ¼",
            "å¥½å§": "å¥½å•¦", "çœŸçš„": "çœŸçš„", "ä¸ºä»€ä¹ˆ": "ç‚ºä»€éº¼"
        }
        for cn, tw in replacements.items():
            text = text.replace(cn, tw)
        return text

    def generate_response(self, text, image_url=None):
        system_prompt = PersonalityManager.get_prompt(self.personality)
        history = list(self.memory.history)
        formatted_history = []
        for item in history:
            formatted_history.append({"role": "user", "content": item['user']})
            formatted_history.append({"role": "assistant", "content": item['ai']})

        conversation = Conversation(text, past_user_inputs=[item['content'] for item in formatted_history if item['role'] == 'user'], generated_responses=[item['content'] for item in formatted_history if item['role'] == 'assistant'])
        conversation.system_prompt = system_prompt
        output = self.conversation_pipeline(conversation)
        response = output.generated_responses[-1]

        localized = self._localize_text(response)
        self.memory.add_interaction(text, localized)
        return localized

    def text_to_speech(self, text):
        """èªéŸ³åˆæˆæ–¹æ³•"""
        communicate = edge_tts.Communicate(
            text,
            voice='zh-TW-HsiaoChenNeural',
            rate="+15%",
            pitch="+30Hz"
        )
        communicate.save_sync("output.mp3")
        print(f"\n[ä¸»æ’­èªéŸ³] {text}")